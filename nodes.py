"""
ComfyUI SAM3 Custom Nodes - Official API Implementation
"""

import torch
import numpy as np
from typing import Tuple, Dict, Any, List, Optional

try:
    import folder_paths
except ImportError:
    class folder_paths:
        models_dir = "models"

from .sam3_utils import (
    SAM3ImageSegmenter,
    DepthEstimator,
    convert_to_segs
)
from .sam3d_body_utils import SAM3DBodyWrapper


class SAM3ModelLoader:
    """Load SAM3 model with auto-download"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "device": (["cuda", "cpu"], {"default": "cuda"}),
            }
        }

    RETURN_TYPES = ("SAM3_MODEL",)
    RETURN_NAMES = ("sam3_model",)
    FUNCTION = "load_model"
    CATEGORY = "SAM3"
    DESCRIPTION = "Load SAM3 model. Auto-downloads if not present. Requires HF authentication."

    def load_model(self, device: str) -> Tuple[Dict[str, Any]]:
        """Load SAM3 model"""
        try:
            # Load segmenter
            segmenter = SAM3ImageSegmenter(device=device)

            model_dict = {
                "segmenter": segmenter,
                "device": device
            }

            return (model_dict,)

        except Exception as e:
            error_msg = f"SAM3 Model Loading Error:\n{str(e)}"
            print(f"[SAM3] ERROR: {error_msg}")
            raise RuntimeError(error_msg)


class SAM3Segmentation:
    """SAM3 Segmentation with Impact Pack SEGS output"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "sam3_model": ("SAM3_MODEL",),
                "image": ("IMAGE",),
                "mode": (["text", "points_from_mask", "auto"], {"default": "text"}),
                "threshold": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05
                }),
            },
            "optional": {
                "text_prompt": ("STRING", {
                    "default": "object",
                    "multiline": False
                }),
                "point_mask": ("MASK",),
                "num_points": ("INT", {
                    "default": 5,
                    "min": 1,
                    "max": 20,
                    "step": 1
                }),
            }
        }

    RETURN_TYPES = ("SEGS", "IMAGE", "MASK")
    RETURN_NAMES = ("segs", "visualization", "combined_mask")
    FUNCTION = "segment"
    CATEGORY = "SAM3"
    DESCRIPTION = "SAM3 segmentation with text or point prompts. Output: Impact Pack SEGS format."

    def segment(
        self,
        sam3_model: Dict[str, Any],
        image: torch.Tensor,
        mode: str,
        threshold: float = 0.5,
        text_prompt: str = "object",
        point_mask: Optional[torch.Tensor] = None,
        num_points: int = 5
    ) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:
        """Perform segmentation"""
        try:
            segmenter = sam3_model["segmenter"]

            # Handle batch
            if len(image.shape) == 4:
                img = image[0]
            else:
                img = image

            h, w, c = img.shape

            # Segment based on mode
            if mode == "text":
                if not text_prompt or text_prompt.strip() == "":
                    text_prompt = "object"

                results = segmenter.segment_with_text(img, text_prompt, threshold)
                label = f"text:{text_prompt}"

            elif mode == "points_from_mask":
                if point_mask is None:
                    raise ValueError("point_mask required for points_from_mask mode")

                # Extract points
                mask = point_mask[0] if len(point_mask.shape) == 3 else point_mask
                points = segmenter.extract_points_from_mask(mask, num_points)

                if len(points) == 0:
                    # Empty result
                    empty_segs = ((w, h), [])
                    empty_mask = torch.zeros((1, h, w), dtype=torch.float32)
                    return (empty_segs, image, empty_mask)

                results = segmenter.segment_with_points(img, points)
                label = "points"

            else:  # auto
                if text_prompt and text_prompt.strip() != "":
                    results = segmenter.segment_with_text(img, text_prompt, threshold)
                    label = f"auto:text:{text_prompt}"
                elif point_mask is not None:
                    mask = point_mask[0] if len(point_mask.shape) == 3 else point_mask
                    points = segmenter.extract_points_from_mask(mask, num_points)
                    if len(points) > 0:
                        results = segmenter.segment_with_points(img, points)
                        label = "auto:points"
                    else:
                        results = {"masks": [], "boxes": [], "scores": []}
                        label = "auto:empty"
                else:
                    results = {"masks": [], "boxes": [], "scores": []}
                    label = "auto:empty"

            # Convert to SEGS
            segs = convert_to_segs(
                results["masks"],
                results["boxes"],
                results["scores"],
                (h, w),
                label
            )

            # Create combined mask
            combined_mask = torch.zeros((h, w), dtype=torch.float32)
            for mask in results["masks"]:
                mask_2d = self._ensure_2d_mask(mask, (h, w))
                combined_mask = torch.maximum(combined_mask, mask_2d)

            # Create visualization
            vis_image = self._create_visualization(img, results["masks"])

            combined_mask = combined_mask.unsqueeze(0)  # [1, H, W]

            return (segs, vis_image, combined_mask)

        except Exception as e:
            error_msg = f"SAM3 Segmentation Error:\n{str(e)}"
            print(f"[SAM3] ERROR: {error_msg}")
            raise RuntimeError(error_msg)

    def _ensure_2d_mask(self, mask: torch.Tensor, target_size: Tuple[int, int]) -> torch.Tensor:
        """Ensure mask is 2D and correct size"""
        if isinstance(mask, torch.Tensor):
            mask_2d = mask.cpu()
        else:
            mask_2d = torch.from_numpy(np.array(mask))

        # Reduce dimensions
        while len(mask_2d.shape) > 2:
            mask_2d = mask_2d.squeeze(0) if mask_2d.shape[0] == 1 else mask_2d[0]

        # Resize if needed
        if mask_2d.shape != target_size:
            mask_2d = torch.nn.functional.interpolate(
                mask_2d.unsqueeze(0).unsqueeze(0),
                size=target_size,
                mode="nearest"
            ).squeeze()

        return mask_2d.float()

    def _create_visualization(self, image: torch.Tensor, masks: List[torch.Tensor]) -> torch.Tensor:
        """Create colored visualization of masks"""
        img_np = image.cpu().numpy()
        h, w = img_np.shape[:2]

        overlay = np.zeros_like(img_np)

        for i, mask in enumerate(masks):
            mask_2d = self._ensure_2d_mask(mask, (h, w)).numpy()

            # Generate color
            color = np.array([
                ((i * 67) % 256) / 255.0,
                ((i * 113) % 256) / 255.0,
                ((i * 197) % 256) / 255.0
            ])

            mask_3d = np.stack([mask_2d] * 3, axis=-1)
            overlay += mask_3d * color

        # Blend
        result = img_np * 0.5 + overlay * 0.5
        result = np.clip(result, 0, 1)

        return torch.from_numpy(result).unsqueeze(0)


class SAM3DepthMap:
    """Generate depth maps for images or segments"""

    def __init__(self):
        self.depth_estimator = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "mode": (["full_image", "per_segment"], {"default": "full_image"}),
                "normalize": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "segs": ("SEGS",),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("depth_image", "depth_mask")
    FUNCTION = "generate_depth"
    CATEGORY = "SAM3"
    DESCRIPTION = "Generate depth maps. per_segment mode requires SEGS input."

    def generate_depth(
        self,
        image: torch.Tensor,
        mode: str,
        normalize: bool = True,
        segs: Optional[Tuple] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Generate depth map"""
        try:
            # Initialize depth estimator
            if self.depth_estimator is None:
                self.depth_estimator = DepthEstimator()

            # Get first image
            img = image[0] if len(image.shape) == 4 else image
            h, w, c = img.shape

            if mode == "full_image":
                depth_map = self.depth_estimator.estimate_depth(img)

            else:  # per_segment
                if segs is None:
                    raise ValueError("SEGS required for per_segment mode")

                (img_w, img_h), segs_list = segs

                if len(segs_list) == 0:
                    depth_map = self.depth_estimator.estimate_depth(img)
                else:
                    depth_map = torch.zeros((h, w), dtype=torch.float32)

                    for seg in segs_list:
                        cropped_mask, crop_region, bbox, label, confidence = seg
                        x, y, crop_w, crop_h = crop_region

                        # Create full mask
                        full_mask = torch.zeros((h, w), dtype=torch.float32)

                        # Resize cropped mask
                        if cropped_mask.shape != (crop_h, crop_w):
                            resized_mask = torch.nn.functional.interpolate(
                                cropped_mask.unsqueeze(0).unsqueeze(0),
                                size=(crop_h, crop_w),
                                mode="nearest"
                            ).squeeze()
                        else:
                            resized_mask = cropped_mask

                        # Place mask
                        end_y = min(y + crop_h, h)
                        end_x = min(x + crop_w, w)
                        actual_h = end_y - y
                        actual_w = end_x - x

                        full_mask[y:end_y, x:end_x] = resized_mask[:actual_h, :actual_w]

                        # Generate depth for segment
                        seg_depth = self.depth_estimator.estimate_depth(img, full_mask)
                        depth_map = torch.maximum(depth_map, seg_depth)

            # Normalize
            if normalize:
                depth_min = depth_map.min()
                depth_max = depth_map.max()
                if depth_max > depth_min:
                    depth_map = (depth_map - depth_min) / (depth_max - depth_min)

            # Convert to image [1, H, W, C]
            depth_image = depth_map.unsqueeze(-1).repeat(1, 1, 3).unsqueeze(0)

            # Also return as mask [1, H, W]
            depth_mask = depth_map.unsqueeze(0)

            return (depth_image, depth_mask)

        except Exception as e:
            error_msg = f"Depth Generation Error:\n{str(e)}"
            print(f"[SAM3] ERROR: {error_msg}")
            raise RuntimeError(error_msg)


class SAM3DBodyModelLoader:
    """Load SAM 3D Body model"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_type": (["dinov3", "vith"], {"default": "dinov3"}),
                "device": (["cuda", "cpu"], {"default": "cuda"}),
            }
        }

    RETURN_TYPES = ("SAM3_BODY_MODEL",)
    RETURN_NAMES = ("sam3d_body_model",)
    FUNCTION = "load_model"
    CATEGORY = "SAM3"
    DESCRIPTION = "Load SAM 3D Body model. Auto-downloads from HF if needed."

    def load_model(self, model_type: str, device: str) -> Tuple[Any]:
        wrapper = SAM3DBodyWrapper(device=device)
        wrapper.load_model(model_type=model_type)
        return (wrapper,)

class SAM3DBodyRun:
    """Run SAM 3D Body on an image"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "sam3d_body_model": ("SAM3_BODY_MODEL",),
                "image": ("IMAGE",),
                "bbox_threshold": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.05}),
                "use_mask": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("visualization", "mesh_mask")
    FUNCTION = "run_inference"
    CATEGORY = "SAM3"
    DESCRIPTION = "Run SAM 3D Body inference to recover 3D human mesh."

    def run_inference(self, sam3d_body_model, image, bbox_threshold, use_mask):
        # Handle batch
        if len(image.shape) == 4:
            img = image[0]
        else:
            img = image
            
        # Convert tensor to numpy [H, W, 3] (0-255)
        img_np = (img.cpu().numpy() * 255).astype(np.uint8)
        
        vis_img, mask = sam3d_body_model.process_image(img_np, bbox_thr=bbox_threshold, use_mask=use_mask)
        
        # Convert vis_img back to tensor [1, H, W, 3] (0-1)
        vis_tensor = torch.from_numpy(vis_img).float() / 255.0
        vis_tensor = vis_tensor.unsqueeze(0)
        
        # Convert mask to tensor [1, H, W]
        mask_tensor = torch.from_numpy(mask).float().unsqueeze(0)
        
        return (vis_tensor, mask_tensor)

# Node registration
NODE_CLASS_MAPPINGS = {
    "SAM3ModelLoader": SAM3ModelLoader,
    "SAM3Segmentation": SAM3Segmentation,
    "SAM3DepthMap": SAM3DepthMap,
    "SAM3DBodyModelLoader": SAM3DBodyModelLoader,
    "SAM3DBodyRun": SAM3DBodyRun,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SAM3ModelLoader": "SAM3 Model Loader",
    "SAM3Segmentation": "SAM3 Segmentation",
    "SAM3DepthMap": "SAM3 Depth Map",
    "SAM3DBodyModelLoader": "SAM 3D Body Model Loader",
    "SAM3DBodyRun": "SAM 3D Body Run",
}
